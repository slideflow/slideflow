


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Multiple-Instance Learning (MIL) &mdash; slideflow 2.3.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Self-Supervised Learning (SSL)" href="../ssl/" />
    <link rel="prev" title="Uncertainty Quantification" href="../uq/" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-43E5QNVXH2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}

    gtag('js', new Date());

    gtag('config', 'G-43E5QNVXH2');
  </script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://slideflow.dev" aria-label="Slideflow"></a>

      <div class="main-menu">
        <ul>
          <li class="active">
            <a href="https://slideflow.dev">Docs</a>
          </li>

          <li>
            <a href="https://slideflow.dev/tutorial1/">Tutorials</a>
          </li>

          <li>
            <a href="https://github.com/jamesdolezal/slideflow">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  2.3
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search/" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation/">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../project_setup/">Setting up a Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets_and_val/">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../slide_processing/">Slide Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/">Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../evaluation/">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../posthoc/">Layer Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uq/">Uncertainty Quantification</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Multiple-Instance Learning (MIL)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ssl/">Self-Supervised Learning (SSL)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stylegan/">Generative Networks (GANs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../saliency/">Saliency Maps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../segmentation/">Tissue Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cellseg/">Cell Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_loops/">Custom Training Loops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../studio/">Slideflow Studio: Live Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting/">Troubleshooting</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tfrecords/">TFRecords: Reading and Writing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataloaders/">Dataloaders: Sampling and Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../custom_extractors/">Custom Feature Extractors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tile_labels/">Strong Supervision with Tile Labels</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../slideflow/">slideflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../project/">slideflow.Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset/">slideflow.Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset_features/">slideflow.DatasetFeatures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../heatmap/">slideflow.Heatmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_params/">slideflow.ModelParams</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mosaic/">slideflow.Mosaic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../slidemap/">slideflow.SlideMap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../biscuit/">slideflow.biscuit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../slideflow_cellseg/">slideflow.cellseg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../io/">slideflow.io</a></li>
<li class="toctree-l1"><a class="reference internal" href="../io_tensorflow/">slideflow.io.tensorflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../io_torch/">slideflow.io.torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gan/">slideflow.gan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../grad/">slideflow.grad</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mil_module/">slideflow.mil</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model/">slideflow.model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_tensorflow/">slideflow.model.tensorflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_torch/">slideflow.model.torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../norm/">slideflow.norm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../simclr/">slideflow.simclr</a></li>
<li class="toctree-l1"><a class="reference internal" href="../slide/">slideflow.slide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../slide_qc/">slideflow.slide.qc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stats/">slideflow.stats</a></li>
<li class="toctree-l1"><a class="reference internal" href="../util/">slideflow.util</a></li>
<li class="toctree-l1"><a class="reference internal" href="../studio_module/">slideflow.studio</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorial1/">Tutorial 1: Model training (simple)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial2/">Tutorial 2: Model training (advanced)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial3/">Tutorial 3: Using a custom architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial4/">Tutorial 4: Model evaluation &amp; heatmaps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial5/">Tutorial 5: Creating a mosaic map</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial6/">Tutorial 6: Custom slide filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial7/">Tutorial 7: Training with custom augmentations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial8/">Tutorial 8: Multiple-Instance Learning</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Multiple-Instance Learning (MIL)</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/mil.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="multiple-instance-learning-mil">
<span id="mil"></span><h1>Multiple-Instance Learning (MIL)<a class="headerlink" href="#multiple-instance-learning-mil" title="Permalink to this heading">¶</a></h1>
<p>In addition to standard tile-based neural networks, Slideflow also supports training multiple-instance learning (MIL) models. Several architectures are available, including <a class="reference external" href="https://github.com/AMLab-Amsterdam/AttentionDeepMIL">attention-based MIL</a> (<code class="docutils literal notranslate"><span class="pre">&quot;Attention_MIL&quot;</span></code>), <a class="reference external" href="https://github.com/mahmoodlab/CLAM">CLAM</a> (<code class="docutils literal notranslate"><span class="pre">&quot;CLAM_SB&quot;,</span></code> <code class="docutils literal notranslate"><span class="pre">&quot;CLAM_MB&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;MIL_fc&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;MIL_fc_mc&quot;</span></code>), <a class="reference external" href="https://github.com/szc19990412/TransMIL">TransMIL</a> (<code class="docutils literal notranslate"><span class="pre">&quot;TransMIL&quot;</span></code>), and <a class="reference external" href="https://github.com/peng-lab/HistoBistro">HistoBistro Transformer</a> (<code class="docutils literal notranslate"><span class="pre">&quot;bistro.transformer&quot;</span></code>). Custom architectures can also be trained. MIL training requires PyTorch.</p>
<p>Skip to <a class="reference internal" href="../tutorial8/#tutorial8"><span class="std std-ref">Tutorial 8: Multiple-Instance Learning</span></a> for a complete example of MIL training.</p>
<section id="generating-features">
<h2>Generating features<a class="headerlink" href="#generating-features" title="Permalink to this heading">¶</a></h2>
<p>The first step in MIL model development is generating features from image tiles. Many types of feature extractors can be used, including imagenet-pretrained models, models finetuned in Slideflow, histology-specific pretrained feature extractors (such as CTransPath or RetCCL), or fine-tuned SSL models.  In all cases, feature extractors are built with <a class="reference internal" href="../model/#slideflow.model.build_feature_extractor" title="slideflow.model.build_feature_extractor"><code class="xref py py-func docutils literal notranslate"><span class="pre">slideflow.model.build_feature_extractor()</span></code></a>, and features are generated for a dataset using either with <a class="reference internal" href="../posthoc/#activations"><span class="std std-ref">slideflow.DatasetFeatures.to_torch()</span></a> or <a class="reference internal" href="../project/#slideflow.Project.generate_feature_bags" title="slideflow.Project.generate_feature_bags"><code class="xref py py-meth docutils literal notranslate"><span class="pre">slideflow.Project.generate_feature_bags()</span></code></a>.</p>
<section id="pretrained-feature-extractor">
<h3>Pretrained Feature Extractor<a class="headerlink" href="#pretrained-feature-extractor" title="Permalink to this heading">¶</a></h3>
<p>Slideflow includes several pathology-specific pretrained feature extractors:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/Xiyue-Wang/TransPath">CTransPath</a></p></li>
<li><p><a class="reference external" href="https://github.com/Xiyue-Wang/RetCCL">RetCCL</a></p></li>
<li><p><a class="reference external" href="https://github.com/owkin/HistoSSLscaling">HistoSSL</a></p></li>
<li><p><a class="reference external" href="https://github.com/PathologyFoundation/plip">PLIP</a></p></li>
</ul>
<p>Use <a class="reference internal" href="../model/#slideflow.model.build_feature_extractor" title="slideflow.model.build_feature_extractor"><code class="xref py py-func docutils literal notranslate"><span class="pre">slideflow.model.build_feature_extractor()</span></code></a> to build one of these feature extractors by name. Weights for these pretrained networks will be automatically downloaded.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ctranspath</span> <span class="o">=</span> <span class="n">build_feature_extractor</span><span class="p">(</span><span class="s1">&#39;ctranspath&#39;</span><span class="p">,</span> <span class="n">tile_px</span><span class="o">=</span><span class="mi">299</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="imagenet-features">
<h3>ImageNet Features<a class="headerlink" href="#imagenet-features" title="Permalink to this heading">¶</a></h3>
<p>To calculate features from an ImageNet-pretrained network, first build an imagenet feature extractor with <a class="reference internal" href="../model/#slideflow.model.build_feature_extractor" title="slideflow.model.build_feature_extractor"><code class="xref py py-func docutils literal notranslate"><span class="pre">slideflow.model.build_feature_extractor()</span></code></a>. The first argument should be the name of an architecture followed by <code class="docutils literal notranslate"><span class="pre">_imagenet</span></code>, and the expected tile size should be passed to the keyword argument <code class="docutils literal notranslate"><span class="pre">tile_px</span></code>. You can optionally specify the layer from which to generate features with the <code class="docutils literal notranslate"><span class="pre">layers</span></code> argument; if not provided, it will default to calculating features from post-convolutional layer activations. For example, to build a ResNet50 feature extractor for images at 299 x 299 pixels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">slideflow.model</span> <span class="kn">import</span> <span class="n">build_feature_extractor</span>

<span class="n">resnet50</span> <span class="o">=</span> <span class="n">build_feature_extractor</span><span class="p">(</span>
    <span class="s1">&#39;resnet50_imagenet&#39;</span><span class="p">,</span>
    <span class="n">tile_px</span><span class="o">=</span><span class="mi">299</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This will calculate features using activations from the post-convolutional layer. You can also concatenate activations from multiple neural network layers and apply pooling for layers with 2D output shapes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">resnet50</span> <span class="o">=</span> <span class="n">build_feature_extractor</span><span class="p">(</span>
    <span class="s1">&#39;resnet50_imagenet&#39;</span><span class="p">,</span>
    <span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;conv1_relu&#39;</span><span class="p">,</span> <span class="s1">&#39;conv3_block1_2_relu&#39;</span><span class="p">],</span>
    <span class="n">pooling</span><span class="o">=</span><span class="s1">&#39;avg&#39;</span><span class="p">,</span>
    <span class="n">tile_px</span><span class="o">=</span><span class="mi">299</span>
<span class="p">)</span>
</pre></div>
</div>
<p>If a model architecture is available in both the Tensorflow and PyTorch backends, Slideflow will default to using the active backend. You can manually set the feature extractor backend using <code class="docutils literal notranslate"><span class="pre">backend</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a PyTorch feature extractor</span>
<span class="n">extractor</span> <span class="o">=</span> <span class="n">build_feature_extractor</span><span class="p">(</span>
    <span class="s1">&#39;resnet50_imagenet&#39;</span><span class="p">,</span>
    <span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;layer2.0.conv1&#39;</span><span class="p">,</span> <span class="s1">&#39;layer3.1.conv2&#39;</span><span class="p">],</span>
    <span class="n">pooling</span><span class="o">=</span><span class="s1">&#39;avg&#39;</span><span class="p">,</span>
    <span class="n">tile_px</span><span class="o">=</span><span class="mi">299</span><span class="p">,</span>
    <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;torch&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You can view all available feature extractors with <a class="reference internal" href="../model/#slideflow.model.list_extractors" title="slideflow.model.list_extractors"><code class="xref py py-func docutils literal notranslate"><span class="pre">slideflow.model.list_extractors()</span></code></a>.</p>
</section>
<section id="features-from-finetuned-model">
<h3>Features from Finetuned Model<a class="headerlink" href="#features-from-finetuned-model" title="Permalink to this heading">¶</a></h3>
<p>You can also calculate features from any model trained in Slideflow. The first argument to <code class="docutils literal notranslate"><span class="pre">build_feature_extractor()</span></code> should be the path of the trained model.  You can optionally specify the layer at which to calculate activations using the <code class="docutils literal notranslate"><span class="pre">layers</span></code> keyword argument. If not specified, activations are calculated at the post-convolutional layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate features from trained model.</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">build_feature_extractor</span><span class="p">(</span>
    <span class="s1">&#39;/path/to/model&#39;</span><span class="p">,</span>
    <span class="n">layers</span><span class="o">=</span><span class="s1">&#39;sepconv3_bn&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="self-supervised-learning">
<h3>Self-Supervised Learning<a class="headerlink" href="#self-supervised-learning" title="Permalink to this heading">¶</a></h3>
<p>Finally, you can also generate features from a trained <a class="reference internal" href="../ssl/#simclr-ssl"><span class="std std-ref">self-supervised learning</span></a> model (either <a class="reference external" href="https://github.com/jamesdolezal/simclr">SimCLR</a> or <a class="reference external" href="https://github.com/jamesdolezal/dinov2">DinoV2</a>).</p>
<p>For SimCLR models, use <code class="docutils literal notranslate"><span class="pre">'simclr'</span></code> as the first argument to <code class="docutils literal notranslate"><span class="pre">build_feature_extractor()</span></code>, and pass the path to a saved model (or saved checkpoint file) via the keyword argument <code class="docutils literal notranslate"><span class="pre">ckpt</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">simclr</span> <span class="o">=</span> <span class="n">build_feature_extractor</span><span class="p">(</span>
    <span class="s1">&#39;simclr&#39;</span><span class="p">,</span>
    <span class="n">ckpt</span><span class="o">=</span><span class="s1">&#39;/path/to/simclr.ckpt&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For DinoV2 models, use <code class="docutils literal notranslate"><span class="pre">'dinov2'</span></code> as the first argument, and pass the model configuration YAML file to <code class="docutils literal notranslate"><span class="pre">cfg</span></code> and the teacher checkpoint weights to <code class="docutils literal notranslate"><span class="pre">weights</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dinov2</span> <span class="o">=</span> <span class="n">build_feature_extractor</span><span class="p">(</span>
    <span class="s1">&#39;dinov2&#39;</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;/path/to/teacher_checkpoint.pth&#39;</span><span class="p">,</span>
    <span class="n">cfg</span><span class="o">=</span><span class="s1">&#39;/path/to/config.yaml&#39;</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="exporting-features">
<span id="bags"></span><h3>Exporting Features<a class="headerlink" href="#exporting-features" title="Permalink to this heading">¶</a></h3>
<p>Once you have prepared a feature extractor, features can be generated for a dataset and exported to disk for later use. Pass a feature extractor to the first argument of <a class="reference internal" href="../project/#slideflow.Project.generate_feature_bags" title="slideflow.Project.generate_feature_bags"><code class="xref py py-meth docutils literal notranslate"><span class="pre">slideflow.Project.generate_feature_bags()</span></code></a>, with a <a class="reference internal" href="../dataset/#slideflow.Dataset" title="slideflow.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">slideflow.Dataset</span></code></a> as the second argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load a project and dataset.</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">sf</span><span class="o">.</span><span class="n">Project</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">dataset</span><span class="p">(</span><span class="n">tile_px</span><span class="o">=</span><span class="mi">299</span><span class="p">,</span> <span class="n">tile_um</span><span class="o">=</span><span class="mi">302</span><span class="p">)</span>

<span class="c1"># Create a feature extractor.</span>
<span class="n">ctranspath</span> <span class="o">=</span> <span class="n">build_feature_extractor</span><span class="p">(</span><span class="s1">&#39;ctranspath&#39;</span><span class="p">,</span> <span class="n">tile_px</span><span class="o">=</span><span class="mi">299</span><span class="p">)</span>

<span class="c1"># Calculate &amp; export feature bags.</span>
<span class="n">P</span><span class="o">.</span><span class="n">generate_feature_bags</span><span class="p">(</span><span class="n">ctranspath</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are generating features from a SimCLR model trained with stain normalization,
you should specify the stain normalizer using the <code class="docutils literal notranslate"><span class="pre">normalizer</span></code> argument to <a class="reference internal" href="../project/#slideflow.Project.generate_feature_bags" title="slideflow.Project.generate_feature_bags"><code class="xref py py-meth docutils literal notranslate"><span class="pre">slideflow.Project.generate_feature_bags()</span></code></a> or <a class="reference internal" href="../dataset_features/#slideflow.DatasetFeatures" title="slideflow.DatasetFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">slideflow.DatasetFeatures</span></code></a>.</p>
</div>
<p>Features are calculated for slides in batches, keeping memory usage low. By default, features are saved to disk in a directory named <code class="docutils literal notranslate"><span class="pre">pt_files</span></code> within the project directory, but you can override the destination directory using the <code class="docutils literal notranslate"><span class="pre">outdir</span></code> argument.</p>
<p>Alternatively, you can calculate features for a dataset using <a class="reference internal" href="../dataset_features/#slideflow.DatasetFeatures" title="slideflow.DatasetFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">slideflow.DatasetFeatures</span></code></a> and the <code class="docutils literal notranslate"><span class="pre">.to_torch()</span></code> method.  This will calculate features for your entire dataset at once, which may require a large amount of memory. The first argument should be the feature extractor, and the second argument should be a <a class="reference internal" href="../dataset/#slideflow.Dataset" title="slideflow.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">slideflow.Dataset</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate features for the entire dataset.</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">sf</span><span class="o">.</span><span class="n">DatasetFeatures</span><span class="p">(</span><span class="n">ctranspath</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Export feature bags.</span>
<span class="n">features</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="s1">&#39;/path/to/bag_directory/&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Using <a class="reference internal" href="../dataset_features/#slideflow.DatasetFeatures" title="slideflow.DatasetFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">slideflow.DatasetFeatures</span></code></a> directly may result in a large amount of memory usage, particularly for sizable datasets. When generating feature bags for training MIL models, it is recommended to use <a class="reference internal" href="../project/#slideflow.Project.generate_feature_bags" title="slideflow.Project.generate_feature_bags"><code class="xref py py-meth docutils literal notranslate"><span class="pre">slideflow.Project.generate_feature_bags()</span></code></a> instead.</p>
</div>
<p>Feature “bags” are PyTorch tensors of features for all images in a slide, saved to disk as <code class="docutils literal notranslate"><span class="pre">.pt</span></code> files. These bags are used to train MIL models. Bags can be manually loaded and inspected using <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;/path/to/bag.pt&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bag</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2310, 768])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bag</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float32</span>
</pre></div>
</div>
<p>When image features are exported for a dataset, the feature extractor configuration is saved to <code class="docutils literal notranslate"><span class="pre">bags_config.json</span></code> in the same directory as the exported features. This configuration file can be used to rebuild the feature extractor. An example file is shown below.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w"> </span><span class="nt">&quot;extractor&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;class&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;slideflow.model.extractors.ctranspath.CTransPathFeatures&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;kwargs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;center_crop&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">  </span><span class="p">}</span>
<span class="w"> </span><span class="p">},</span>
<span class="w"> </span><span class="nt">&quot;normalizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;method&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;macenko&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;fit&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;stain_matrix_target&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">[</span>
<span class="w">     </span><span class="mf">0.5062568187713623</span><span class="p">,</span>
<span class="w">     </span><span class="mf">0.22186939418315887</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="p">[</span>
<span class="w">     </span><span class="mf">0.7532230615615845</span><span class="p">,</span>
<span class="w">     </span><span class="mf">0.8652154803276062</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="p">[</span>
<span class="w">     </span><span class="mf">0.4069173336029053</span><span class="p">,</span>
<span class="w">     </span><span class="mf">0.42241501808166504</span>
<span class="w">    </span><span class="p">]</span>
<span class="w">   </span><span class="p">],</span>
<span class="w">   </span><span class="nt">&quot;target_concentrations&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="mf">1.7656903266906738</span><span class="p">,</span>
<span class="w">    </span><span class="mf">1.2797492742538452</span>
<span class="w">   </span><span class="p">]</span>
<span class="w">  </span><span class="p">}</span>
<span class="w"> </span><span class="p">},</span>
<span class="w"> </span><span class="nt">&quot;num_features&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2048</span><span class="p">,</span>
<span class="w"> </span><span class="nt">&quot;tile_px&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">299</span><span class="p">,</span>
<span class="w"> </span><span class="nt">&quot;tile_um&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">302</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The feature extractor can be manually rebuilt using <a class="reference internal" href="../model/#slideflow.model.rebuild_extractor" title="slideflow.model.rebuild_extractor"><code class="xref py py-func docutils literal notranslate"><span class="pre">slideflow.model.rebuild_extractor()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">slideflow.model</span> <span class="kn">import</span> <span class="n">rebuild_extractor</span>

<span class="c1"># Recreate the feature extractor</span>
<span class="c1"># and stain normalizer, if applicable</span>
<span class="n">extractor</span><span class="p">,</span> <span class="n">normalizer</span> <span class="o">=</span> <span class="n">rebuild_extractor</span><span class="p">(</span><span class="s1">&#39;/path/to/bags_config.json&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="license-citation">
<h3>License &amp; Citation<a class="headerlink" href="#license-citation" title="Permalink to this heading">¶</a></h3>
<p>Licensing and citation information for the pretrained feature extractors is accessible with the <code class="docutils literal notranslate"><span class="pre">.license</span></code> and <code class="docutils literal notranslate"><span class="pre">.citation</span></code> attributes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ctranspath</span><span class="o">.</span><span class="n">license</span>
<span class="go">&#39;GNU General Public License v3.0&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ctranspath</span><span class="o">.</span><span class="n">citation</span><span class="p">)</span>

<span class="go">@{wang2022,</span>
<span class="go">  title={Transformer-based Unsupervised Contrastive Learning for Histopathological Image Classification},</span>
<span class="go">  author={Wang, Xiyue and Yang, Sen and Zhang, Jun and Wang, Minghui and Zhang, Jing  and Yang, Wei and Huang, Junzhou  and Han, Xiao},</span>
<span class="go">  journal={Medical Image Analysis},</span>
<span class="go">  year={2022},</span>
<span class="go">  publisher={Elsevier}</span>
<span class="go">}</span>
</pre></div>
</div>
</section>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this heading">¶</a></h2>
<section id="model-configuration">
<h3>Model Configuration<a class="headerlink" href="#model-configuration" title="Permalink to this heading">¶</a></h3>
<p>To train an MIL model on exported features, first prepare an MIL configuration using <a class="reference internal" href="../mil_module/#slideflow.mil.mil_config" title="slideflow.mil.mil_config"><code class="xref py py-func docutils literal notranslate"><span class="pre">slideflow.mil.mil_config()</span></code></a>.</p>
<p>The first argument to this function is the model architecture (which can be a name or a custom <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> model), and the remaining arguments are used to configure the training process (including learning rate and epochs).</p>
<p>By default, training is executed using <a class="reference external" href="https://docs.fast.ai/">FastAI</a> with <a class="reference external" href="https://arxiv.org/pdf/1803.09820.pdf%E5%92%8CSylvain">1cycle learning rate scheduling</a>. Available models out-of-the-box include <a class="reference external" href="https://github.com/AMLab-Amsterdam/AttentionDeepMIL">attention-based MIL</a> (<code class="docutils literal notranslate"><span class="pre">&quot;Attention_MIL&quot;</span></code>), <a class="reference external" href="https://github.com/mahmoodlab/CLAM">CLAM</a> (<code class="docutils literal notranslate"><span class="pre">&quot;CLAM_SB&quot;,</span></code> <code class="docutils literal notranslate"><span class="pre">&quot;CLAM_MB&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;MIL_fc&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;MIL_fc_mc&quot;</span></code>), <a class="reference external" href="https://github.com/szc19990412/TransMIL">transformer MIL</a> (<code class="docutils literal notranslate"><span class="pre">&quot;TransMIL&quot;</span></code>), and <a class="reference external" href="https://github.com/peng-lab/HistoBistro">HistoBistro Transformer</a> (<code class="docutils literal notranslate"><span class="pre">&quot;bistro.transformer&quot;</span></code>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">slideflow</span> <span class="k">as</span> <span class="nn">sf</span>
<span class="kn">from</span> <span class="nn">slideflow.mil</span> <span class="kn">import</span> <span class="n">mil_config</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">mil_config</span><span class="p">(</span><span class="s1">&#39;attention_mil&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</pre></div>
</div>
<p>Custom MIL models can also be trained with this API. Import a custom MIL model as a PyTorch module, and pass this as the first argument to <a class="reference internal" href="../mil_module/#slideflow.mil.mil_config" title="slideflow.mil.mil_config"><code class="xref py py-func docutils literal notranslate"><span class="pre">slideflow.mil.mil_config()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">slideflow</span> <span class="k">as</span> <span class="nn">sf</span>
<span class="kn">from</span> <span class="nn">slideflow.mil</span> <span class="kn">import</span> <span class="n">mil_config</span>
<span class="kn">from</span> <span class="nn">my_module</span> <span class="kn">import</span> <span class="n">CustomMIL</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">mil_config</span><span class="p">(</span><span class="n">CustomMIL</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="legacy-clam-trainer">
<h3>Legacy CLAM Trainer<a class="headerlink" href="#legacy-clam-trainer" title="Permalink to this heading">¶</a></h3>
<p>In addition to the FastAI trainer, CLAM models can be trained using the <a class="reference external" href="https://github.com/mahmoodlab/CLAM">original</a> CLAM training loop. This trainer has been modified, cleaned, and included as a submodule in Slideflow. This legacy trainer can be used for CLAM models by setting <code class="docutils literal notranslate"><span class="pre">trainer='clam'</span></code> for an MIL configuration:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">mil_config</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">trainer</span><span class="o">=</span><span class="s1">&#39;clam&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-an-mil-model">
<h3>Training an MIL Model<a class="headerlink" href="#training-an-mil-model" title="Permalink to this heading">¶</a></h3>
<p>Next, prepare a <a class="reference internal" href="../datasets_and_val/#datasets-and-validation"><span class="std std-ref">training and validation dataset</span></a> and use <a class="reference internal" href="../project/#slideflow.Project.train_mil" title="slideflow.Project.train_mil"><code class="xref py py-func docutils literal notranslate"><span class="pre">slideflow.Project.train_mil()</span></code></a> to start training. For example, to train a model using three-fold cross-validation to the outcome “HPV_status”:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>

<span class="c1"># Prepare a project and dataset</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">sf</span><span class="o">.</span><span class="n">Project</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">full_dataset</span> <span class="o">=</span> <span class="n">dataset</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">dataset</span><span class="p">(</span><span class="n">tile_px</span><span class="o">=</span><span class="mi">299</span><span class="p">,</span> <span class="n">tile_um</span><span class="o">=</span><span class="mi">302</span><span class="p">)</span>

<span class="c1"># Split the dataset using three-fold, site-preserved cross-validation</span>
<span class="n">splits</span> <span class="o">=</span> <span class="n">full_dataset</span><span class="o">.</span><span class="n">kfold_split</span><span class="p">(</span>
    <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="s1">&#39;HPV_status&#39;</span><span class="p">,</span>
    <span class="n">preserved_site</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># Train on each cross-fold</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span>
    <span class="n">P</span><span class="o">.</span><span class="n">train_mil</span><span class="p">(</span>
        <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
        <span class="n">outcomes</span><span class="o">=</span><span class="s1">&#39;HPV_status&#39;</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="o">=</span><span class="n">train</span><span class="p">,</span>
        <span class="n">val_dataset</span><span class="o">=</span><span class="n">val</span><span class="p">,</span>
        <span class="n">bags</span><span class="o">=</span><span class="s1">&#39;/path/to/bag_directory&#39;</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Model training statistics, including validation performance (AUROC, AP) and predictions on the validation dataset, will be saved in an <code class="docutils literal notranslate"><span class="pre">mil</span></code> subfolder within the main project directory.</p>
<p>If you are training an attention-based MIL model (<code class="docutils literal notranslate"><span class="pre">attention_mil</span></code>, <code class="docutils literal notranslate"><span class="pre">clam_sb</span></code>, <code class="docutils literal notranslate"><span class="pre">clam_mb</span></code>), heatmaps of attention can be generated for each slide in the validation dataset by using the argument <code class="docutils literal notranslate"><span class="pre">attention_heatmaps=True</span></code>. You can customize these heatmaps with <code class="docutils literal notranslate"><span class="pre">interpolation</span></code> and <code class="docutils literal notranslate"><span class="pre">cmap</span></code> arguments to control the heatmap interpolation and colormap, respectively.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate attention heatmaps,</span>
<span class="c1"># using the &#39;magma&#39; colormap and no interpolation.</span>
<span class="n">P</span><span class="o">.</span><span class="n">train_mil</span><span class="p">(</span>
    <span class="n">attention_heatmaps</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;magma&#39;</span><span class="p">,</span>
    <span class="n">interpolation</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Hyperparameters, model configuration, and feature extractor information is logged to <code class="docutils literal notranslate"><span class="pre">mil_params.json</span></code> in the model directory. This file also contains information about the input and output shapes of the MIL network and outcome labels. An example file is shown below.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w"> </span><span class="nt">&quot;trainer&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;fastai&quot;</span><span class="p">,</span>
<span class="w"> </span><span class="nt">&quot;params&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>

<span class="w"> </span><span class="p">},</span>
<span class="w"> </span><span class="nt">&quot;outcomes&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;histology&quot;</span><span class="p">,</span>
<span class="w"> </span><span class="nt">&quot;outcome_labels&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;0&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Adenocarcinoma&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;1&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Squamous&quot;</span>
<span class="w"> </span><span class="p">},</span>
<span class="w"> </span><span class="nt">&quot;bags&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/mnt/data/projects/example_project/bags/simclr-263510/&quot;</span><span class="p">,</span>
<span class="w"> </span><span class="nt">&quot;input_shape&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1024</span><span class="p">,</span>
<span class="w"> </span><span class="nt">&quot;output_shape&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w"> </span><span class="nt">&quot;bags_encoder&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;extractor&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;class&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;slideflow.model.extractors.simclr.SimCLR_Features&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;kwargs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;center_crop&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;ckpt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/mnt/data/projects/example_project/simclr/00001-EXAMPLE/ckpt-263510.ckpt&quot;</span>
<span class="w">   </span><span class="p">}</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;normalizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;num_features&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1024</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;tile_px&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">299</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;tile_um&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">302</span>
<span class="w"> </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="multi-magnification-mil">
<span id="multimag"></span><h3>Multi-Magnification MIL<a class="headerlink" href="#multi-magnification-mil" title="Permalink to this heading">¶</a></h3>
<p>Slideflow 2.2 introduced a multi-magnification, multi-modal MIL model, <code class="docutils literal notranslate"><span class="pre">MultiModal_Attention_MIL</span></code> (<code class="docutils literal notranslate"><span class="pre">&quot;mm_attention_mil&quot;</span></code>). This late-fusion multimodal model is based on standard attention-based MIL, but accepts multiple input modalities (e.g., multiple magnifications) simultaneously. Each input modality is processed by a separate encoder network and a separate attention module. The attention-weighted features from each modality are then concatenated and passed to a fully-connected layer.</p>
<p>Multimodal models are trained using the same API as standard MIL models. Modalities are specified using the <code class="docutils literal notranslate"><span class="pre">bags</span></code> argument to <a class="reference internal" href="../project/#slideflow.Project.train_mil" title="slideflow.Project.train_mil"><code class="xref py py-func docutils literal notranslate"><span class="pre">slideflow.Project.train_mil()</span></code></a>, where the number of modes is determined by the number of bag directories provided. Within each bag directory, bags should be generated using the same feature extractor and at the same magnification, but feature extractors and magnifications can vary between bag directories.</p>
<p>For example, to train a multimodal model using two magnifications, you would pass two bag paths to the model. In this case, the <code class="docutils literal notranslate"><span class="pre">/path/to/bags_10x</span></code> directory contains bags generated from a 10x feature extractor, and the <code class="docutils literal notranslate"><span class="pre">/path/to/bags_40x</span></code> directory contains bags generated from a 40x feature extractor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configure a multimodal MIL model.</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">mil_config</span><span class="p">(</span><span class="s1">&#39;mm_attention_mil&#39;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="c1"># Set the bags paths for each modality.</span>
<span class="n">bags_10x</span> <span class="o">=</span> <span class="s1">&#39;/path/to/bags_10x&#39;</span>
<span class="n">bags_40x</span> <span class="o">=</span> <span class="s1">&#39;/path/to/bags_40x&#39;</span>

<span class="n">P</span><span class="o">.</span><span class="n">train_mil</span><span class="p">(</span>
    <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">outcomes</span><span class="o">=</span><span class="s1">&#39;HPV_status&#39;</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train</span><span class="p">,</span>
    <span class="n">val_dataset</span><span class="o">=</span><span class="n">val</span><span class="p">,</span>
    <span class="n">bags</span><span class="o">=</span><span class="p">[</span><span class="n">bags_10x</span><span class="p">,</span> <span class="n">bags_40x</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You can use any number of modalities, and the feature extractors for each modality can be different. For example, you could train a multimodal model using features from a custom SimCLR model at 5x and features from a pretrained CTransPath model at 20x.</p>
<p>The feature extractors used for each modality, as specified in the <code class="docutils literal notranslate"><span class="pre">bags_config.json</span></code> files in the bag directories, will be logged in the final <code class="docutils literal notranslate"><span class="pre">mil_params.json</span></code> file. Multimodal MIL models can be interactively viewed in <a class="reference internal" href="../studio/#studio"><span class="std std-ref">Slideflow Studio</span></a>, allowing you to visualize the attention weights for each modality separately.</p>
</section>
</section>
<section id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this heading">¶</a></h2>
<p>To evaluate a saved MIL model on an external dataset, first extract features from a dataset, then use <a class="reference internal" href="../project/#slideflow.Project.evaluate_mil" title="slideflow.Project.evaluate_mil"><code class="xref py py-func docutils literal notranslate"><span class="pre">slideflow.Project.evaluate_mil()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">slideflow</span> <span class="k">as</span> <span class="nn">sf</span>
<span class="kn">from</span> <span class="nn">slideflow.model</span> <span class="kn">import</span> <span class="n">build_feature_extractor</span>

<span class="c1"># Prepare a project and dataset</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">sf</span><span class="o">.</span><span class="n">Project</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">dataset</span><span class="p">(</span><span class="n">tile_px</span><span class="o">=</span><span class="mi">299</span><span class="p">,</span> <span class="n">tile_um</span><span class="o">=</span><span class="mi">302</span><span class="p">)</span>

<span class="c1"># Generate features using CTransPath</span>
<span class="n">ctranspath</span> <span class="o">=</span> <span class="n">build_feature_extractor</span><span class="p">(</span><span class="s1">&#39;ctranspath&#39;</span><span class="p">,</span> <span class="n">tile_px</span><span class="o">=</span><span class="mi">299</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">sf</span><span class="o">.</span><span class="n">DatasetFeatures</span><span class="p">(</span><span class="n">ctranspath</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">features</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="s1">&#39;/path/to/bag_directory&#39;</span><span class="p">)</span>

<span class="c1"># Evaluate a saved MIL model</span>
<span class="n">P</span><span class="o">.</span><span class="n">evaluate_mil</span><span class="p">(</span>
    <span class="s1">&#39;/path/to/saved_model&#39;</span>
    <span class="n">outcomes</span><span class="o">=</span><span class="s1">&#39;HPV_status&#39;</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">bags</span><span class="o">=</span><span class="s1">&#39;/path/to/bag_directory&#39;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>As with training, attention heatmaps can be generated for attention-based MIL models with the argument <code class="docutils literal notranslate"><span class="pre">attention_heatmaps=True</span></code>, and these can be customized using <code class="docutils literal notranslate"><span class="pre">cmap</span></code> and <code class="docutils literal notranslate"><span class="pre">interpolation</span></code> arguments.</p>
<img alt="../_images/att_heatmap.jpg" src="../_images/att_heatmap.jpg" />
</section>
<section id="single-slide-inference">
<h2>Single-Slide Inference<a class="headerlink" href="#single-slide-inference" title="Permalink to this heading">¶</a></h2>
<p>Predictions can also be generated for individual slides, without requiring the user to manually generate feature bags. Use <code class="xref py py-func docutils literal notranslate"><span class="pre">slideflow.model.predict_slide()</span></code> to generate predictions for a single slide. The first argument is th path to the saved MIL model (a directory containing <code class="docutils literal notranslate"><span class="pre">mil_params.json</span></code>), and the second argument can either be a path to a slide or a loaded <code class="xref py py-class docutils literal notranslate"><span class="pre">sf.WSI</span></code> object.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">slideflow.mil</span> <span class="kn">import</span> <span class="n">predict_slide</span>
<span class="kn">from</span> <span class="nn">slideflow.slide</span> <span class="kn">import</span> <span class="n">qc</span>

<span class="c1"># Load a slide and apply Otsu thresholding</span>
<span class="n">slide</span> <span class="o">=</span> <span class="s1">&#39;/path/to/slide.svs&#39;</span>
<span class="n">wsi</span> <span class="o">=</span> <span class="n">sf</span><span class="o">.</span><span class="n">WSI</span><span class="p">(</span><span class="n">slide</span><span class="p">,</span> <span class="n">tile_px</span><span class="o">=</span><span class="mi">299</span><span class="p">,</span> <span class="n">tile_um</span><span class="o">=</span><span class="mi">302</span><span class="p">)</span>
<span class="n">wsi</span><span class="o">.</span><span class="n">qc</span><span class="p">(</span><span class="n">qc</span><span class="o">.</span><span class="n">Otsu</span><span class="p">())</span>

<span class="c1"># Calculate predictions and attention heatmap</span>
<span class="n">model</span> <span class="o">=</span> <span class="s1">&#39;/path/to/mil_model&#39;</span>
<span class="n">y_pred</span><span class="p">,</span> <span class="n">y_att</span> <span class="o">=</span> <span class="n">predict_slide</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">wsi</span><span class="p">)</span>
</pre></div>
</div>
<p>The function will return a tuple of predictions and attention heatmaps. If the model is not attention-based, the attention heatmap will be <code class="docutils literal notranslate"><span class="pre">None</span></code>. To calculate attention for a model, set <code class="docutils literal notranslate"><span class="pre">attention=True</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_att</span> <span class="o">=</span> <span class="n">predict_slide</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">slide</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The returned attention values will be a masked <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code> with the same shape as the slide tile extraction grid. Unused tiles will have masked attention values.</p>
</section>
<section id="visualizing-attention-heatmaps">
<h2>Visualizing Attention Heatmaps<a class="headerlink" href="#visualizing-attention-heatmaps" title="Permalink to this heading">¶</a></h2>
<p>Attention heatmaps can be interactively visualized in Slideflow Studio by enabling the Multiple-Instance Learning extension (new in Slideflow 2.1.0). This extension is discussed in more detail in the <a class="reference internal" href="../studio/#extensions"><span class="std std-ref">Extensions</span></a> section.</p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../ssl/" class="btn btn-neutral float-right" title="Self-Supervised Learning (SSL)" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="../uq/" class="btn btn-neutral" title="Uncertainty Quantification" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, James M Dolezal.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Multiple-Instance Learning (MIL)</a><ul>
<li><a class="reference internal" href="#generating-features">Generating features</a><ul>
<li><a class="reference internal" href="#pretrained-feature-extractor">Pretrained Feature Extractor</a></li>
<li><a class="reference internal" href="#imagenet-features">ImageNet Features</a></li>
<li><a class="reference internal" href="#features-from-finetuned-model">Features from Finetuned Model</a></li>
<li><a class="reference internal" href="#self-supervised-learning">Self-Supervised Learning</a></li>
<li><a class="reference internal" href="#exporting-features">Exporting Features</a></li>
<li><a class="reference internal" href="#license-citation">License &amp; Citation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#training">Training</a><ul>
<li><a class="reference internal" href="#model-configuration">Model Configuration</a></li>
<li><a class="reference internal" href="#legacy-clam-trainer">Legacy CLAM Trainer</a></li>
<li><a class="reference internal" href="#training-an-mil-model">Training an MIL Model</a></li>
<li><a class="reference internal" href="#multi-magnification-mil">Multi-Magnification MIL</a></li>
</ul>
</li>
<li><a class="reference internal" href="#evaluation">Evaluation</a></li>
<li><a class="reference internal" href="#single-slide-inference">Single-Slide Inference</a></li>
<li><a class="reference internal" href="#visualizing-attention-heatmaps">Visualizing Attention Heatmaps</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/sphinx_highlight.js"></script>
     

  
  <script type="text/javascript" src="../_static/js/vendor/jquery-3.6.3.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://slideflow.dev">Docs</a>
          </li>

          <li>
            <a href="https://slideflow.dev/tutorial1/">Tutorials</a>
          </li>

          <li>
            <a href="https://github.com/jamesdolezal/slideflow">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script script type="text/javascript">
    var collapsedSections = [];
  </script>

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>